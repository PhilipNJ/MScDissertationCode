{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:10.791123Z",
     "start_time": "2024-07-13T18:47:09.549494Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:10.837278Z",
     "start_time": "2024-07-13T18:47:10.792506Z"
    }
   },
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    def __init__(self, data, window_size, max_hold_days=12):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.current_step = 0\n",
    "        self.inventory = []\n",
    "        self.cash = 10000  # Starting cash\n",
    "        self.max_inventory = 10  # Maximum inventory\n",
    "        self.state_size = window_size * 5  # Number of features (Open, High, Low, Close, Volume)\n",
    "        self.max_hold_days = max_hold_days\n",
    "        self.holding_periods = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.inventory = []\n",
    "        self.cash = 10000\n",
    "        self.holding_periods = []\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return self.data[self.current_step:self.current_step + self.window_size].flatten()\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data[self.current_step, 3]  # Close price\n",
    "        reward = 0\n",
    "\n",
    "        if action == 0:  # Buy\n",
    "            if len(self.inventory) < self.max_inventory and self.cash >= float(current_price):\n",
    "                self.inventory.append(float(current_price))\n",
    "                self.cash -= float(current_price)\n",
    "                self.holding_periods.append(0)\n",
    "        elif action == 1:  # Sell\n",
    "            if len(self.inventory) > 0:\n",
    "                bought_price = self.inventory.pop(0)\n",
    "                reward = float(current_price) - bought_price\n",
    "                self.cash += float(current_price)\n",
    "                self.holding_periods.pop(0)\n",
    "        # Hold (action == 2) doesn't change inventory or cash\n",
    "\n",
    "        # Update holding periods and enforce max holding days\n",
    "        self.holding_periods = [days + 1 for days in self.holding_periods]\n",
    "        while len(self.holding_periods) > 0 and self.holding_periods[0] > self.max_hold_days:\n",
    "            # Force sell stocks held for more than max_hold_days\n",
    "            bought_price = self.inventory.pop(0)\n",
    "            reward += float(current_price) - bought_price  # Include forced sell in reward\n",
    "            self.cash += float(current_price)\n",
    "            self.holding_periods.pop(0)\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - self.window_size\n",
    "        next_state = self._get_state() if not done else None\n",
    "\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:10.847343Z",
     "start_time": "2024-07-13T18:47:10.838117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(data, window_size):\n",
    "    data = data[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "    n = len(data) - window_size\n",
    "    states = np.array([data[i:i + window_size] for i in range(n)])\n",
    "    return states\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_pickle('data/SP500.pkl')\n",
    "window_size = 9\n",
    "data = prepare_data(data, window_size)\n",
    "train_size = int(len(data) * 0.7)\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:10.850856Z",
     "start_time": "2024-07-13T18:47:10.848712Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:10.855012Z",
     "start_time": "2024-07-13T18:47:10.851555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "state_size = window_size * 5\n",
    "action_size = 3\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "memory_size = 10000\n",
    "target_update = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:10.858027Z",
     "start_time": "2024-07-13T18:47:10.855852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Memory for experience replay\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:11.454196Z",
     "start_time": "2024-07-13T18:47:10.858606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize models and optimizer\n",
    "policy_net = DQN(state_size, action_size)\n",
    "target_net = DQN(state_size, action_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:11.457219Z",
     "start_time": "2024-07-13T18:47:11.454971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "    batch_state = torch.tensor(batch_state, dtype=torch.float32)\n",
    "    batch_action = torch.tensor(batch_action)\n",
    "    batch_reward = torch.tensor(batch_reward, dtype=torch.float32)\n",
    "    batch_next_state = torch.tensor(batch_next_state, dtype=torch.float32)\n",
    "\n",
    "    current_q_values = policy_net(batch_state).gather(1, batch_action.unsqueeze(1))\n",
    "    next_q_values = target_net(batch_next_state).max(1)[0].detach()\n",
    "    expected_q_values = batch_reward + (gamma * next_q_values)\n",
    "\n",
    "    loss = nn.MSELoss()(current_q_values.squeeze(), expected_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T18:47:14.662410Z",
     "start_time": "2024-07-13T18:47:14.400362Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     14\u001b[0m         action \u001b[38;5;241m=\u001b[39m policy_net(state_tensor)\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 16\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     17\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     18\u001b[0m memory\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state)\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mTradingEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Buy\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minventory) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_inventory \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcash \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(current_price):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minventory\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(current_price))\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcash \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(current_price)\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_episodes = 100\n",
    "env = TradingEnvironment(train_data, window_size)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(len(train_data) - window_size):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state_tensor).argmax().item()\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        memory.push(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    if episode % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print(f'Episode {episode}, Total Reward: {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(policy_net.state_dict(), 'dqn_trading_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "env = TradingEnvironment(test_data, window_size)\n",
    "state = env.reset()\n",
    "total_profit = 0\n",
    "while True:\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state_tensor).argmax().item()\n",
    "\n",
    "    next_state, reward, done = env.step(action)\n",
    "    total_profit += reward\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f'Total Profit: {total_profit}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
