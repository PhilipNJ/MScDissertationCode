{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T18:47:10.791123Z",
          "start_time": "2024-07-13T18:47:09.549494Z"
        },
        "id": "MPdxPDpj4txb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g84tlHgg4txc"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_pickle('data/SP500.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VcY7cFSd8tVI"
      },
      "outputs": [],
      "source": [
        "df_train = df_raw.iloc[:len(df_raw)*8//10]\n",
        "df_test = df_raw.iloc[len(df_raw)*8//10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4026, 6) (1007, 6)\n",
            "5033\n",
            "5033\n"
          ]
        }
      ],
      "source": [
        "print(df_train.shape, df_test.shape)\n",
        "print(df_train.shape[0] + df_test.shape[0])\n",
        "print(df_raw.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GsD6ct_n4txd"
      },
      "outputs": [],
      "source": [
        "def create_states(df, window_size=9):\n",
        "    states = []\n",
        "    for i in range(window_size, len(df)):\n",
        "        state = df.iloc[i-window_size:i].values\n",
        "        states.append(state)\n",
        "    return np.array(states)\n",
        "\n",
        "states = create_states(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7i3_T5PP4txd"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Example dimensions\n",
        "input_dim = states.shape[1] * states.shape[2]  # window_size * number of features\n",
        "output_dim = 3  # buy, sell, hold\n",
        "\n",
        "# Initialize the model and move it to the device\n",
        "model = DQN(input_dim, output_dim) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RsmRPqSf4txd"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Initialize replay memory\n",
        "memory = ReplayMemory(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, lr=0.0001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = ReplayMemory(10000)\n",
        "        self.gamma = gamma    # discount rate\n",
        "        self.epsilon = epsilon   # exploration rate\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = DQN(state_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.last_buy_index = -float('inf')  # Track the last buy index\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        act_values = self.model(state)\n",
        "        return torch.argmax(act_values[0]).item()\n",
        "    \n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = self.memory.sample(batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * torch.max(self.model(next_state)[0]).item()\n",
        "            target_f = self.model(state)\n",
        "            target_f[0][action] = target\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.criterion(target_f, self.model(state))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-07-16 20:10:02,687] A new study created in memory with name: no-name-98b8fcc4-cd02-4d2e-b1ee-b3abc1b6484f\n"
          ]
        }
      ],
      "source": [
        "# Define the objective function\n",
        "def objective(trial, states=states):\n",
        "    state_size = states.shape[1] * states.shape[2]\n",
        "    action_size = 3\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "    gamma = trial.suggest_uniform('gamma', 0.8, 0.999)\n",
        "    epsilon = trial.suggest_uniform('epsilon', 0.8, 1.0)\n",
        "    epsilon_min = trial.suggest_uniform('epsilon_min', 0.01, 0.1)\n",
        "    epsilon_decay = trial.suggest_uniform('epsilon_decay', 0.9, 0.999)\n",
        "\n",
        "    # Initialize the agent with suggested hyperparameters\n",
        "    agent = DQNAgent(state_size, action_size, lr, gamma, epsilon, epsilon_min, epsilon_decay)\n",
        "\n",
        "    # Train the agent on the training set\n",
        "    episodes = 100\n",
        "    batch_size = 32\n",
        "    total_rewards = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = states[0]\n",
        "        total_reward = 0\n",
        "        last_action = 'Hold'\n",
        "        days_since_last_buy = 0\n",
        "        has_open_position = False\n",
        "\n",
        "        for time in range(1, len(states)):\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # Ensure sell only after buy and max 11 days between buy and sell\n",
        "            if has_open_position:\n",
        "                days_since_last_buy += 1\n",
        "\n",
        "            if action == 0:  # Sell\n",
        "                if not has_open_position:  # Can't sell if no open position\n",
        "                    action = 2  # Change to Hold if not previously bought\n",
        "                else:\n",
        "                    has_open_position = False  # Sell the open position\n",
        "                    days_since_last_buy = 0  # Reset days since last buy\n",
        "\n",
        "            if action == 1:  # Buy\n",
        "                if has_open_position:  # Can't buy if already have an open position\n",
        "                    action = 2  # Change to Hold if already bought\n",
        "                else:\n",
        "                    has_open_position = True  # Buy, opening a new position\n",
        "                    days_since_last_buy = 0  # Reset days since last buy\n",
        "\n",
        "            # Force a sell if more than 11 days since last buy\n",
        "            if has_open_position and days_since_last_buy > 11:\n",
        "                action = 0  # Force a sell\n",
        "\n",
        "            # Update last action and reset days since last buy if sell or buy happens\n",
        "            if action == 1:  # Buy\n",
        "                days_since_last_buy = 0\n",
        "                has_open_position = True\n",
        "            if action == 0:  # Sell\n",
        "                has_open_position = False\n",
        "\n",
        "            next_state = states[time]\n",
        "            reward = next_state[-1][3] - state[-1][3]  # Example: Reward based on price difference\n",
        "            agent.remember(state, action, reward, next_state, done=(time == len(states) - 1))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if time == len(states) - 1:\n",
        "                total_rewards.append(total_reward)\n",
        "                break\n",
        "\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "\n",
        "        print(f\"Iteration {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "    return np.mean(total_rewards)\n",
        "\n",
        "# Optimize hyperparameters with Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "print(f\"Best hyperparameters: {study.best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the agent\n",
        "state_size = states.shape[1] * states.shape[2]\n",
        "action_size = 3\n",
        "agent = DQNAgent(state_size, action_size, lr=0.0005, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# episodes = 1\n",
        "# batch_size = 32\n",
        "# Episode = []\n",
        "# Time = []\n",
        "# Reward = []\n",
        "# Total_Reward = []\n",
        "# Action = []\n",
        "# next_price = []\n",
        "# prev_price = []\n",
        "# last_action = 'Hold'\n",
        "# days_since_last_buy = 0\n",
        "# has_open_position = False\n",
        "\n",
        "# for e in range(episodes):\n",
        "#     state = states[0]\n",
        "#     total_reward = 0\n",
        "#     last_action = 'Hold'\n",
        "#     days_since_last_buy = 0\n",
        "#     has_open_position = False  # Track if there's an open position\n",
        "    \n",
        "#     for time in range(1, len(states)):\n",
        "#         action = agent.act(state)\n",
        "\n",
        "#         # Ensure sell only after buy and max 11 days between buy and sell\n",
        "#         if has_open_position:\n",
        "#             days_since_last_buy += 1\n",
        "\n",
        "#         if action == 0:  # Sell\n",
        "#             if not has_open_position:  # Can't sell if no open position\n",
        "#                 action = 2  # Change to Hold if not previously bought\n",
        "#             else:\n",
        "#                 has_open_position = False  # Sell the open position\n",
        "#                 days_since_last_buy = 0  # Reset days since last buy\n",
        "\n",
        "#         if action == 1:  # Buy\n",
        "#             if has_open_position:  # Can't buy if already have an open position\n",
        "#                 action = 2  # Change to Hold if already bought\n",
        "#             else:\n",
        "#                 has_open_position = True  # Buy, opening a new position\n",
        "#                 days_since_last_buy = 0  # Reset days since last buy\n",
        "\n",
        "#         # Force a sell if more than 11 days since last buy\n",
        "#         if has_open_position and days_since_last_buy > 11:\n",
        "#             action = 0  # Force a sell\n",
        "\n",
        "#         # Update last action and reset days since last buy if sell or buy happens\n",
        "#         if action == 1:  # Buy\n",
        "#             days_since_last_buy = 0\n",
        "#             has_open_position = True\n",
        "#         if action == 0:  # Sell\n",
        "#             has_open_position = False\n",
        "\n",
        "#         next_state = states[time]\n",
        "#         reward = next_state[-1][3] - state[-1][3]  # Example: Reward based on price difference\n",
        "#         next_price.append(next_state[-1][3])\n",
        "#         prev_price.append(state[-1][3])\n",
        "#         done = time == len(states) - 1\n",
        "#         agent.remember(state, action, reward, next_state, done)\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "#         Episode.append(e+1)\n",
        "#         Time.append(time)\n",
        "#         Reward.append(reward)\n",
        "#         Total_Reward.append(total_reward)\n",
        "#         Action.append(action)\n",
        "#         if done:\n",
        "#             print(f\"Episode {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "#             break\n",
        "#         if len(agent.memory) > batch_size:\n",
        "#             agent.replay(batch_size)\n",
        "\n",
        "# log_train = pd.DataFrame({'Episode': Episode, 'Time': Time, 'Reward': Reward, 'Total_Reward': Total_Reward, 'Action': Action, 'Next_Price': next_price, 'Prev_Price': prev_price})\n",
        "# log_train['Action'] = log_train['Action'].map({0: 'Sell', 1: 'Buy', 2: 'Hold'})\n",
        "# log_train['Action'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def evaluate_agent(agent, states):\n",
        "#     state = states[0]\n",
        "#     total_reward = 0\n",
        "#     actions = []\n",
        "#     days_since_last_buy = 0\n",
        "#     has_open_position = False\n",
        "#     last_action = 'Hold'\n",
        "    \n",
        "#     for time in range(1, len(states)):\n",
        "#         action = agent.act(state)\n",
        "\n",
        "#         # Ensure sell only after buy and max 11 days between buy and sell\n",
        "#         if has_open_position:\n",
        "#             days_since_last_buy += 1\n",
        "\n",
        "#         if action == 0:  # Sell\n",
        "#             if not has_open_position:  # Can't sell if no open position\n",
        "#                 action = 2  # Change to Hold if not previously bought\n",
        "#             else:\n",
        "#                 has_open_position = False  # Sell the open position\n",
        "#                 days_since_last_buy = 0  # Reset days since last buy\n",
        "\n",
        "#         if action == 1:  # Buy\n",
        "#             if has_open_position:  # Can't buy if already have an open position\n",
        "#                 action = 2  # Change to Hold if already bought\n",
        "#             else:\n",
        "#                 has_open_position = True  # Buy, opening a new position\n",
        "#                 days_since_last_buy = 0  # Reset days since last buy\n",
        "\n",
        "#         # Force a sell if more than 11 days since last buy\n",
        "#         if has_open_position and days_since_last_buy > 11:\n",
        "#             action = 0  # Force a sell\n",
        "\n",
        "#         # Update last action and reset days since last buy if sell or buy happens\n",
        "#         if action == 1:  # Buy\n",
        "#             days_since_last_buy = 0\n",
        "#             has_open_position = True\n",
        "#         if action == 0:  # Sell\n",
        "#             has_open_position = False\n",
        "\n",
        "#         next_state = states[time]\n",
        "#         reward = next_state[-1][3] - state[-1][3]  # Example: Reward based on price difference\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "#         actions.append(action)\n",
        "\n",
        "#     return total_reward, actions\n",
        "\n",
        "# # Assume `validation_states` is the validation dataset prepared similarly to the training dataset\n",
        "# test_states = create_states(df_test)\n",
        "# total_reward, actions = evaluate_agent(agent, test_states)\n",
        "# print(f\"Total Reward on Test Data: {total_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def plot_actions(df, title='Trading Actions'):\n",
        "#     plt.figure(figsize=(15, 7))\n",
        "#     plt.plot(df['Close'], label='Close Price', color='blue')\n",
        "#     buy_signals = df[df['Action'] == 'Buy']\n",
        "#     sell_signals = df[df['Action'] == 'Sell']\n",
        "#     hold_signals = df[df['Action'] == 'Hold']\n",
        "#     plt.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', label='Buy', alpha=1)\n",
        "#     plt.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', label='Sell', alpha=1)\n",
        "#     plt.scatter(hold_signals.index, hold_signals['Close'], marker='o', color='orange', label='Hold', alpha=0.5)\n",
        "#     plt.title(title)\n",
        "#     plt.xlabel('Date')\n",
        "#     plt.ylabel('Price')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "# # Visualize actions on training data\n",
        "# plot_actions(df_train, title='Training Data Actions')\n",
        "\n",
        "# #visualize actions on validation data\n",
        "# plot_actions(df_validation, title='Validation Data Actions')\n",
        "\n",
        "# # Visualize actions on test data\n",
        "# plot_actions(df_test, title='Test Data Actions')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOzewn7X0Q0"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
