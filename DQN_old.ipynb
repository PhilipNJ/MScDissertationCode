{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-07-13T18:47:10.791123Z",
          "start_time": "2024-07-13T18:47:09.549494Z"
        },
        "id": "MPdxPDpj4txb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g84tlHgg4txc"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_pickle('data/SP500.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VcY7cFSd8tVI"
      },
      "outputs": [],
      "source": [
        "df_train = df_raw.iloc[:len(df_raw)*6//10]\n",
        "df_val = df_raw.iloc[len(df_raw)*6//10:len(df_raw)*8//10]\n",
        "df_test = df_raw.iloc[len(df_raw)*8//10:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3019, 6) (1007, 6) (1007, 6)\n",
            "5033\n",
            "5033\n"
          ]
        }
      ],
      "source": [
        "print(df_train.shape, df_val.shape, df_test.shape)\n",
        "print(df_train.shape[0]+df_val.shape[0]+df_test.shape[0])\n",
        "print(df_raw.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GsD6ct_n4txd"
      },
      "outputs": [],
      "source": [
        "def create_states(df, window_size=10):\n",
        "    states = []\n",
        "    for i in range(window_size, len(df)):\n",
        "        state = df.iloc[i-window_size:i].values\n",
        "        states.append(state)\n",
        "    return np.array(states)\n",
        "\n",
        "states = create_states(df_train)\n",
        "validation_states = create_states(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7i3_T5PP4txd"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Example dimensions\n",
        "input_dim = states.shape[1] * states.shape[2]  # window_size * number of features\n",
        "output_dim = 3  # buy, sell, hold\n",
        "\n",
        "# Initialize the model and move it to the device\n",
        "model = DQN(input_dim, output_dim) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RsmRPqSf4txd"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# Initialize replay memory\n",
        "memory = ReplayMemory(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = ReplayMemory(10000)\n",
        "        self.gamma = 0.99    # discount rate\n",
        "        self.epsilon = 1.0   # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = DQN(state_size, action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.last_buy_index = -float('inf')  # Track the last buy index\n",
        "    \n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        act_values = self.model(state)\n",
        "        return torch.argmax(act_values[0]).item()\n",
        "    \n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = self.memory.sample(batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * torch.max(self.model(next_state)[0]).item()\n",
        "            target_f = self.model(state)\n",
        "            target_f[0][action] = target\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.criterion(target_f, self.model(state))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Initialize the agent\n",
        "state_size = states.shape[1] * states.shape[2]\n",
        "action_size = 3\n",
        "agent = DQNAgent(state_size, action_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1/1, Total Reward: 908.8800048828125\n"
          ]
        }
      ],
      "source": [
        "episodes = 1\n",
        "batch_size = 32\n",
        "Episode = []\n",
        "Time = []\n",
        "Reward = []\n",
        "Total_Reward = []\n",
        "Action = []\n",
        "next_price = []\n",
        "prev_price = []\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = states[0]\n",
        "    total_reward = 0\n",
        "    for time in range(1, len(states)):\n",
        "        action = agent.act(state)\n",
        "        next_state = states[time]\n",
        "        reward = next_state[-1][3] - state[-1][3]  # Example: Reward based on price difference\n",
        "        done = time == len(states) - 1\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward   \n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "            break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hold    1965\n",
              "Buy      620\n",
              "Sell     423\n",
              "Name: Action, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "log_train = pd.DataFrame({'Episode': Episode, 'Time': Time, 'Reward': Reward, 'Total_Reward': Total_Reward, 'Action': Action, 'Next_Price': next_price, 'Prev_Price': prev_price})\n",
        "log_train['Action'] = log_train['Action'].map({0: 'Sell', 1: 'Buy', 2: 'Hold'})\n",
        "log_train['Action'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Episode</th>\n",
              "      <th>Time</th>\n",
              "      <th>Reward</th>\n",
              "      <th>Total_Reward</th>\n",
              "      <th>Action</th>\n",
              "      <th>Next_Price</th>\n",
              "      <th>Prev_Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.669922</td>\n",
              "      <td>0.669922</td>\n",
              "      <td>Sell</td>\n",
              "      <td>1128.839966</td>\n",
              "      <td>1128.170044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5.770020</td>\n",
              "      <td>6.439941</td>\n",
              "      <td>Hold</td>\n",
              "      <td>1134.609985</td>\n",
              "      <td>1128.839966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.209961</td>\n",
              "      <td>7.649902</td>\n",
              "      <td>Sell</td>\n",
              "      <td>1135.819946</td>\n",
              "      <td>1134.609985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-17.669922</td>\n",
              "      <td>-10.020020</td>\n",
              "      <td>Sell</td>\n",
              "      <td>1118.150024</td>\n",
              "      <td>1135.819946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5.939941</td>\n",
              "      <td>-4.080078</td>\n",
              "      <td>Hold</td>\n",
              "      <td>1124.089966</td>\n",
              "      <td>1118.150024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3003</th>\n",
              "      <td>1</td>\n",
              "      <td>3004</td>\n",
              "      <td>2.020020</td>\n",
              "      <td>923.430054</td>\n",
              "      <td>Hold</td>\n",
              "      <td>2051.600098</td>\n",
              "      <td>2049.580078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3004</th>\n",
              "      <td>1</td>\n",
              "      <td>3005</td>\n",
              "      <td>-1.800049</td>\n",
              "      <td>921.630005</td>\n",
              "      <td>Hold</td>\n",
              "      <td>2049.800049</td>\n",
              "      <td>2051.600098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3005</th>\n",
              "      <td>1</td>\n",
              "      <td>3006</td>\n",
              "      <td>-13.090088</td>\n",
              "      <td>908.539917</td>\n",
              "      <td>Hold</td>\n",
              "      <td>2036.709961</td>\n",
              "      <td>2049.800049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3006</th>\n",
              "      <td>1</td>\n",
              "      <td>3007</td>\n",
              "      <td>-0.770020</td>\n",
              "      <td>907.769897</td>\n",
              "      <td>Hold</td>\n",
              "      <td>2035.939941</td>\n",
              "      <td>2036.709961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3007</th>\n",
              "      <td>1</td>\n",
              "      <td>3008</td>\n",
              "      <td>1.110107</td>\n",
              "      <td>908.880005</td>\n",
              "      <td>Hold</td>\n",
              "      <td>2037.050049</td>\n",
              "      <td>2035.939941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3008 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Episode  Time     Reward  Total_Reward Action   Next_Price   Prev_Price\n",
              "0           1     1   0.669922      0.669922   Sell  1128.839966  1128.170044\n",
              "1           1     2   5.770020      6.439941   Hold  1134.609985  1128.839966\n",
              "2           1     3   1.209961      7.649902   Sell  1135.819946  1134.609985\n",
              "3           1     4 -17.669922    -10.020020   Sell  1118.150024  1135.819946\n",
              "4           1     5   5.939941     -4.080078   Hold  1124.089966  1118.150024\n",
              "...       ...   ...        ...           ...    ...          ...          ...\n",
              "3003        1  3004   2.020020    923.430054   Hold  2051.600098  2049.580078\n",
              "3004        1  3005  -1.800049    921.630005   Hold  2049.800049  2051.600098\n",
              "3005        1  3006 -13.090088    908.539917   Hold  2036.709961  2049.800049\n",
              "3006        1  3007  -0.770020    907.769897   Hold  2035.939941  2036.709961\n",
              "3007        1  3008   1.110107    908.880005   Hold  2037.050049  2035.939941\n",
              "\n",
              "[3008 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(log_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvrFfG_t4txd",
        "outputId": "57c2728c-d6d5-414a-aabb-6953c4741a78"
      },
      "outputs": [],
      "source": [
        "# episodes = 50\n",
        "# batch_size = 32\n",
        "# Episode = []\n",
        "# Time = []\n",
        "# Reward = []\n",
        "# Total_Reward = []\n",
        "# Action = []\n",
        "\n",
        "# for e in range(episodes):\n",
        "#     state = states[0]\n",
        "#     total_reward = 0\n",
        "#     for time in range(1, len(states)):\n",
        "#         action = agent.act(state, time)\n",
        "#         next_state = states[time]\n",
        "#         reward = next_state[-1][3] - state[-1][3]  # Example: Reward based on price difference\n",
        "        \n",
        "#         done = time == len(states) - 1 \n",
        "#         agent.remember(state, action, reward, next_state, done)\n",
        "#         state = next_state\n",
        "#         total_reward += reward\n",
        "#         Episode.append(e+1)\n",
        "#         Time.append(time)\n",
        "#         Reward.append(reward)\n",
        "#         Total_Reward.append(total_reward)\n",
        "#         Action.append(action)\n",
        "\n",
        "#         if done:\n",
        "#             print(f\"Episode {e+1}/{episodes}, Total Reward: {total_reward}\")\n",
        "#             break\n",
        "#         if len(agent.memory) > batch_size:\n",
        "#             agent.replay(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_train = pd.DataFrame({'Episode': Episode, 'Time': Time, 'Reward': Reward, 'Total_Reward': Total_Reward, 'Action': Action})\n",
        "log_train['Action'] = log_train['Action'].map({0: 'Sell', 1: 'Buy', 2: 'Hold'})\n",
        "log_train['Action'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5HaoJ5wANk1",
        "outputId": "173e1f6f-1b72-49a7-fd72-ef19c0a47703"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(agent, states):\n",
        "    state = states[0]\n",
        "    total_reward = 0\n",
        "    actions = []\n",
        "    for time in range(1, len(states)):\n",
        "        action = agent.act(state, time)\n",
        "        next_state = states[time]\n",
        "        reward = next_state[-1][3] - state[-1][3]  # Example: Reward based on price difference\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        actions.append(action)\n",
        "    return total_reward, actions\n",
        "\n",
        "# Assume `validation_states` is the validation dataset prepared similarly to the training dataset\n",
        "validation_states = create_states(df_validation)\n",
        "total_reward, actions = evaluate_agent(agent, validation_states)\n",
        "print(f\"Total Reward on Validation Data: {total_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de5fVNJxBB48"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_size, action_size, lr=0.0005, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "cjm9mSy1BFIO",
        "outputId": "1e98170b-7e2d-46cf-dd38-d05acc7a36fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate after fine-tuning\n",
        "total_reward, actions = evaluate_agent(agent, validation_states)\n",
        "print(f\"Total Reward on Validation Data after fine-tuning: {total_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOQ7kHTrBG6i"
      },
      "outputs": [],
      "source": [
        "def predict_actions(agent, states):\n",
        "    actions = []\n",
        "    agent.last_buy_index = -float('inf')  # Reset last buy index\n",
        "    agent.last_action = 2  # Assume starting with 'Hold' (2: Hold)\n",
        "\n",
        "    for current_index, state in enumerate(states):\n",
        "        action = agent.act(state, current_index)\n",
        "        actions.append(action)\n",
        "\n",
        "    return actions\n",
        "\n",
        "\n",
        "# Predict actions for test dataset (assuming test_states is prepared similarly to training states)\n",
        "test_states = create_states(df_test)\n",
        "test_actions = predict_actions(agent, test_states)\n",
        "\n",
        "\n",
        "# Predict actions for test dataset (assuming test_states is prepared similarly to training states)\n",
        "test_states = create_states(df_test)\n",
        "test_actions = predict_actions(agent, test_states)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilkkv9moESFi",
        "outputId": "153b80ef-f398-4638-93e3-da78c0000425"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Mapping actions back to the test dataframe\n",
        "df_test['Action'] = [None] * (len(df_test) - len(test_actions)) + test_actions\n",
        "\n",
        "# Mapping actions (0: Hold, 1: Buy, 2: Sell) to strings for better readability\n",
        "action_map = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
        "df_test['Action'] = df_test['Action'].map(action_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQiiT7yEXArj",
        "outputId": "75389a9c-36e2-4e1f-f153-d3872ef32ce2"
      },
      "outputs": [],
      "source": [
        "df_test['Action'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aiEMGrmOEbKC",
        "outputId": "8cc680d6-3521-4b75-af53-829224502ecf"
      },
      "outputs": [],
      "source": [
        "def plot_actions(df, title='Trading Actions'):\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.plot(df['Close'], label='Close Price', color='blue')\n",
        "    buy_signals = df[df['Action'] == 'Buy']\n",
        "    sell_signals = df[df['Action'] == 'Sell']\n",
        "    hold_signals = df[df['Action'] == 'Hold']\n",
        "    plt.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', label='Buy', alpha=1)\n",
        "    plt.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', label='Sell', alpha=1)\n",
        "    plt.scatter(hold_signals.index, hold_signals['Close'], marker='o', color='orange', label='Hold', alpha=0.5)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize actions on training data\n",
        "plot_actions(df_train, title='Training Data Actions')\n",
        "\n",
        "#visualize actions on validation data\n",
        "plot_actions(df_validation, title='Validation Data Actions')\n",
        "\n",
        "# Visualize actions on test data\n",
        "plot_actions(df_test, title='Test Data Actions')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOzewn7X0Q0"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
